<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Publications | Tianyu Zhang's WebPage</title><link>https://ai.t-zhang.com/publication/</link><atom:link href="https://ai.t-zhang.com/publication/index.xml" rel="self" type="application/rss+xml"/><description>Publications</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><image><url>https://ai.t-zhang.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url><title>Publications</title><link>https://ai.t-zhang.com/publication/</link></image><item><title>STRICT: Stress Test of Rendering Images Containing Text</title><link>https://ai.t-zhang.com/publication/strict/</link><pubDate>Sun, 25 May 2025 00:00:00 +0000</pubDate><guid>https://ai.t-zhang.com/publication/strict/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>&lt;a href="https://proceedings.iclr.cc/paper_files/paper/2025/hash/812e3f4ff9877b318689a22d81172cf3-Abstract-Conference.html" target="_blank" rel="noopener">Paper&lt;/a>&lt;/p></description></item><item><title>Advantage Alignment Algorithms</title><link>https://ai.t-zhang.com/publication/aaa/</link><pubDate>Mon, 05 May 2025 00:00:00 +0000</pubDate><guid>https://ai.t-zhang.com/publication/aaa/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>&lt;a href="https://proceedings.iclr.cc/paper_files/paper/2025/hash/5185aa776fd64ae3b4c6dae1af1066b1-Abstract-Conference.html" target="_blank" rel="noopener">Paper&lt;/a>&lt;/p></description></item><item><title>MuPT: A Generative Symbolic Music Pretrained Transformer</title><link>https://ai.t-zhang.com/publication/mupt/</link><pubDate>Mon, 05 May 2025 00:00:00 +0000</pubDate><guid>https://ai.t-zhang.com/publication/mupt/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>&lt;a href="https://proceedings.iclr.cc/paper_files/paper/2025/hash/73f6f8897896f7bda86ea7d1ebc1dc4f-Abstract-Conference.html" target="_blank" rel="noopener">Paper&lt;/a>&lt;/p></description></item><item><title>MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation</title><link>https://ai.t-zhang.com/publication/map/</link><pubDate>Mon, 28 Apr 2025 00:00:00 +0000</pubDate><guid>https://ai.t-zhang.com/publication/map/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>&lt;a href="https://proceedings.iclr.cc/paper_files/paper/2025/hash/a3724221b623bf1966e92cd266ac6177-Abstract-Conference.html" target="_blank" rel="noopener">Paper&lt;/a>&lt;/p></description></item><item><title>VCR: A Task for Pixel-Level Complex Reasoning in Vision Language Models via Restoring Occluded Text</title><link>https://ai.t-zhang.com/publication/vcr/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://ai.t-zhang.com/publication/vcr/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>&lt;a href="https://proceedings.iclr.cc/paper_files/paper/2025/hash/812e3f4ff9877b318689a22d81172cf3-Abstract-Conference.html" target="_blank" rel="noopener">Paper&lt;/a>&lt;/p></description></item><item><title>BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks</title><link>https://ai.t-zhang.com/publication/bigdocs/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://ai.t-zhang.com/publication/bigdocs/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>&lt;a href="https://proceedings.iclr.cc/paper_files/paper/2025/hash/c4659191ae1e89faa09864c23fa91f31-Abstract-Conference.html" target="_blank" rel="noopener">Paper&lt;/a>&lt;/p></description></item><item><title>Expected flow networks in stochastic environments and two-player zero-sum games</title><link>https://ai.t-zhang.com/publication/expectedflow/</link><pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate><guid>https://ai.t-zhang.com/publication/expectedflow/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>&lt;a href="https://openreview.net/forum?id=uH0FGECSEI" target="_blank" rel="noopener">Paper&lt;/a>&lt;/p></description></item><item><title>Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</title><link>https://ai.t-zhang.com/publication/clap/</link><pubDate>Sat, 08 Apr 2023 00:00:00 +0000</pubDate><guid>https://ai.t-zhang.com/publication/clap/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>&lt;a href="https://arxiv.org/abs/2211.06687" target="_blank" rel="noopener">Paper&lt;/a>&lt;/p></description></item><item><title>Multi-Agent Reinforcement Learning for Fast-Timescale Demand Response of Residential Loads</title><link>https://ai.t-zhang.com/publication/egrid2/</link><pubDate>Fri, 06 Jan 2023 00:00:00 +0000</pubDate><guid>https://ai.t-zhang.com/publication/egrid2/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>&lt;a href="https://arxiv.org/abs/2301.02593" target="_blank" rel="noopener">Paper&lt;/a>&lt;/p></description></item><item><title>Biological Sequence Design with GFlowNets</title><link>https://ai.t-zhang.com/publication/biological-sequence-design-with-gflownets/</link><pubDate>Tue, 28 Jun 2022 00:00:00 +0000</pubDate><guid>https://ai.t-zhang.com/publication/biological-sequence-design-with-gflownets/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>&lt;a href="https://proceedings.mlr.press/v162/jain22a.html" target="_blank" rel="noopener">Paper&lt;/a>&lt;/p></description></item><item><title>(Private)-Retroactive Carbon Pricing [(P)ReCaP]: A Market-based Approach for Climate Finance and Risk Assessment</title><link>https://ai.t-zhang.com/publication/precap/</link><pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate><guid>https://ai.t-zhang.com/publication/precap/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>&lt;a href="https://arxiv.org/abs/2205.00666" target="_blank" rel="noopener">Paper&lt;/a>&lt;/p></description></item><item><title>ClimateGAN: Raising Climate Change Awareness by Generating Images of Floods</title><link>https://ai.t-zhang.com/publication/climategan-raising-climate-change-awareness-by-generating-images-of-floods/</link><pubDate>Wed, 06 Oct 2021 00:00:00 +0000</pubDate><guid>https://ai.t-zhang.com/publication/climategan-raising-climate-change-awareness-by-generating-images-of-floods/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>&lt;a href="https://openreview.net/forum?id=EZNOb_uNpJk" target="_blank" rel="noopener">Paper&lt;/a>&lt;/p></description></item><item><title>A minimal sufficient set of procedures in a bargaining model</title><link>https://ai.t-zhang.com/publication/a-minimal-sufficient-set-of-procedures-in-a-bargaining-model/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://ai.t-zhang.com/publication/a-minimal-sufficient-set-of-procedures-in-a-bargaining-model/</guid><description>&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>&lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0165176517300162" target="_blank" rel="noopener">Paper&lt;/a>&lt;/p></description></item><item><title/><link>https://ai.t-zhang.com/publication/alignvlm/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ai.t-zhang.com/publication/alignvlm/</guid><description>&lt;hr>
&lt;p>title: &amp;lsquo;AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Understanding&amp;rsquo;&lt;/p>
&lt;h1 id="authors">Authors&lt;/h1>
&lt;h1 id="if-you-created-a-profile-for-a-user-eg-the-default-admin-user-write-the-username-folder-name-here">If you created a profile for a user (e.g. the default &lt;code>admin&lt;/code> user), write the username (folder name) here&lt;/h1>
&lt;h1 id="and-it-will-be-replaced-with-their-full-name-and-linked-to-their-profile">and it will be replaced with their full name and linked to their profile.&lt;/h1>
&lt;p>authors:&lt;/p>
&lt;ul>
&lt;li>Ahmed Masry&lt;/li>
&lt;li>Juan A. Rodriguez&lt;/li>
&lt;li>admin&lt;/li>
&lt;li>Suyuchen Wang&lt;/li>
&lt;li>Chao Wang&lt;/li>
&lt;li>Aarash Feizi&lt;/li>
&lt;li>Akshay Kalkunte Suresh&lt;/li>
&lt;li>Abhay Puri&lt;/li>
&lt;li>Xiangru Jian&lt;/li>
&lt;li>Pierre-André Noël&lt;/li>
&lt;li>Sathwik Tejaswi Madhusudhan&lt;/li>
&lt;li>Marco Pedersoli&lt;/li>
&lt;li>Bang Liu&lt;/li>
&lt;li>Nicolas Chapados&lt;/li>
&lt;li>admin&lt;/li>
&lt;li>Enamul Hoque&lt;/li>
&lt;li>Christopher Pal&lt;/li>
&lt;li>Issam H. Laradji&lt;/li>
&lt;li>David Vazquez&lt;/li>
&lt;li>Perouz Taslakian&lt;/li>
&lt;li>Spandana Gella&lt;/li>
&lt;li>Sai Rajeswar&lt;/li>
&lt;/ul>
&lt;h1 id="author-notes-optional">Author notes (optional)&lt;/h1>
&lt;h1 id="author_notes-">author_notes: []&lt;/h1>
&lt;p>date: &amp;lsquo;2025-02-03T00:00:00Z&amp;rsquo;
doi: ''&lt;/p>
&lt;h1 id="schedule-page-publish-date-not-publications-date">Schedule page publish date (NOT publication&amp;rsquo;s date).&lt;/h1>
&lt;p>publishDate: &amp;lsquo;2022-06-28T00:00:00Z&amp;rsquo;&lt;/p>
&lt;h1 id="publication-type">Publication type.&lt;/h1>
&lt;h1 id="legend-0--uncategorized-1--conference-paper-2--journal-article">Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;&lt;/h1>
&lt;h1 id="3--preprint--working-paper-4--report-5--book-6--book-section">3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;&lt;/h1>
&lt;h1 id="7--thesis-8--patent">7 = Thesis; 8 = Patent&lt;/h1>
&lt;p>publication_types: [&amp;lsquo;3&amp;rsquo;]&lt;/p>
&lt;h1 id="publication-name-and-optional-abbreviated-publication-name">Publication name and optional abbreviated publication name.&lt;/h1>
&lt;p>publication: &amp;lsquo;arXiv&amp;rsquo;
publication_short: &amp;lsquo;arXiv&amp;rsquo;&lt;/p>
&lt;p>abstract: Aligning visual features with language embeddings is a key challenge in vision-language models (VLMs). The performance of such models hinges on having a good connector that maps visual features generated by a vision encoder to a shared embedding space with the LLM while preserving semantic similarity. Existing connectors, such as multilayer perceptrons (MLPs), often produce out-of-distribution or noisy inputs, leading to misalignment between the modalities. In this work, we propose a novel vision-text alignment method, AlignVLM, that maps visual features to a weighted average of LLM text embeddings. Our approach leverages the linguistic priors encoded by the LLM to ensure that visual features are mapped to regions of the space that the LLM can effectively interpret. AlignVLM is particularly effective for document understanding tasks, where scanned document images must be accurately mapped to their textual content. Our extensive experiments show that AlignVLM achieves state-of-the-art performance compared to prior alignment methods. We provide further analysis demonstrating improved vision-text feature alignment and robustness to noise.&lt;/p>
&lt;h1 id="-summary-an-optional-shortened-abstract"># Summary. An optional shortened abstract.&lt;/h1>
&lt;h1 id="summary-lorem-ipsum-dolor-sit-amet-consectetur-adipiscing-elit-duis-posuere-tellus-ac-convallis-placerat-proin-tincidunt-magna-sed-ex-sollicitudin-condimentum">summary: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.&lt;/h1>
&lt;p>tags: [Multimodal, Vision-Language Models, Document Understanding, Alignment, Deep Learning]&lt;/p>
&lt;h1 id="display-this-page-in-the-featured-widget">Display this page in the Featured widget?&lt;/h1>
&lt;p>featured: true&lt;/p>
&lt;h1 id="custom-links-uncomment-lines-below">Custom links (uncomment lines below)&lt;/h1>
&lt;h1 id="links">links:&lt;/h1>
&lt;h1 id="--name-custom-link">- name: Custom Link&lt;/h1>
&lt;h1 id="url-httpexampleorg">url: &lt;a href="http://example.org" target="_blank" rel="noopener">http://example.org&lt;/a>&lt;/h1>
&lt;p>url_pdf: &amp;rsquo;'
url_code: &amp;rsquo;'
url_dataset: &amp;rsquo;'
url_poster: &amp;rsquo;'
url_project: &amp;rsquo;'
url_slides: &amp;rsquo;'
url_source: &amp;rsquo;'
url_video: ''&lt;/p>
&lt;h1 id="featured-image">Featured image&lt;/h1>
&lt;h1 id="to-use-add-an-image-named-featuredjpgpng-to-your-pages-folder">To use, add an image named &lt;code>featured.jpg/png&lt;/code> to your page&amp;rsquo;s folder.&lt;/h1>
&lt;h1 id="image">image:&lt;/h1>
&lt;h1 id="caption-image-credit-unsplashhttpsunsplashcomphotosplcdaamflte">caption: &amp;lsquo;Image credit: &lt;a href="https://unsplash.com/photos/pLCdAaMFLTE" target="_blank" rel="noopener">&lt;strong>Unsplash&lt;/strong>&lt;/a>&amp;rsquo;&lt;/h1>
&lt;h1 id="focal_point-">focal_point: ''&lt;/h1>
&lt;h1 id="preview_only-false">preview_only: false&lt;/h1>
&lt;h1 id="associated-projects-optional">Associated Projects (optional).&lt;/h1>
&lt;h1 id="associate-this-publication-with-one-or-more-of-your-projects">Associate this publication with one or more of your projects.&lt;/h1>
&lt;h1 id="simply-enter-your-projects-folder-or-file-name-without-extension">Simply enter your project&amp;rsquo;s folder or file name without extension.&lt;/h1>
&lt;h1 id="eg-internal-project-references-contentprojectinternal-projectindexmd">E.g. &lt;code>internal-project&lt;/code> references &lt;code>content/project/internal-project/index.md&lt;/code>.&lt;/h1>
&lt;h1 id="otherwise-set-projects-">Otherwise, set &lt;code>projects: []&lt;/code>.&lt;/h1>
&lt;h1 id="projects">projects:&lt;/h1>
&lt;h1 id="--dni">- DNI&lt;/h1>
&lt;h1 id="slides-optional">Slides (optional).&lt;/h1>
&lt;h1 id="associate-this-publication-with-markdown-slides">Associate this publication with Markdown slides.&lt;/h1>
&lt;h1 id="simply-enter-your-slide-decks-filename-without-extension">Simply enter your slide deck&amp;rsquo;s filename without extension.&lt;/h1>
&lt;h1 id="eg-slides-example-references-contentslidesexampleindexmd">E.g. &lt;code>slides: &amp;quot;example&amp;quot;&lt;/code> references &lt;code>content/slides/example/index.md&lt;/code>.&lt;/h1>
&lt;h1 id="otherwise-set-slides-">Otherwise, set &lt;code>slides: &amp;quot;&amp;quot;&lt;/code>.&lt;/h1>
&lt;h1 id="slides-example">slides: example&lt;/h1>
&lt;hr>
&lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
&lt;p>&lt;a href="https://arxiv.org/abs/2502.01341" target="_blank" rel="noopener">Paper&lt;/a>&lt;/p></description></item></channel></rss>