<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=author content="Tianyu Zhang"><meta name=description content="Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks -- text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-630K and the proposed model are both available to the public."><link rel=alternate hreflang=en-us href=https://tyz.netlify.app/publication/clap/><meta name=theme-color content="#1565c0"><link rel=stylesheet href="../../css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href="https://main--tyz.netlify.app/css/wowchemy.1052fab8b7700a3dc49ee23683097d66.css"><link rel=manifest href="../../manifest.webmanifest"><link rel=icon type=image/png href="https://main--tyz.netlify.app/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png"><link rel=apple-touch-icon type=image/png href="../../media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png"><link rel=canonical href=https://tyz.netlify.app/publication/clap/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="og:site_name" content="Tianyu Zhang's WebPage"><meta property="og:url" content="https://tyz.netlify.app/publication/clap/"><meta property="og:title" content="Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation  | Tianyu Zhang's WebPage"><meta property="og:description" content="Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks -- text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-630K and the proposed model are both available to the public."><meta property="og:image" content="https://tyz.netlify.app/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://tyz.netlify.app/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-04-08T00:00:00+00:00"><meta property="article:modified_time" content="2023-04-18T14:07:38-04:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://tyz.netlify.app/publication/clap/"},"headline":"Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation ","datePublished":"2023-04-08T00:00:00Z","dateModified":"2023-04-18T14:07:38-04:00","author":{"@type":"Person","name":"Yusong Wu"},"publisher":{"@type":"Organization","name":"Tianyu Zhang's WebPage","logo":{"@type":"ImageObject","url":"https://tyz.netlify.app/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_3.png"}},"description":"Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks -- text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-630K and the proposed model are both available to the public."}</script><title>Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation | Tianyu Zhang's WebPage</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=dae08a903e2735e4b7fbb8ae3db0bf89><script src="../../js/wowchemy-init.min.2ed908358299dd7ab553faae685c746c.js"></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href="index.html#" aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href="../../index.html">Tianyu Zhang's WebPage</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href="../../index.html">Tianyu Zhang's WebPage</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href="../../index.html#about"><span>Home</span></a></li><li class=nav-item><a class=nav-link href="../../index.html#projects"><span>Projects</span></a></li><li class=nav-item><a class=nav-link href="../../index.html#publications"><span>Publications</span></a></li><li class=nav-item><a class=nav-link href="../../index.html#contact"><span>Contact</span></a></li><li class=nav-item><a class=nav-link href="https://main--tyz.netlify.app/uploads/Academic_CV.pdf"><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href="index.html#" aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href="index.html#" class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href="index.html#" class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href="index.html#" class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href="index.html#" class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</h1><div class=article-metadata><div><span>Yusong Wu</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Ke Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>Tianyu Zhang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yuchen Hui</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Taylor Berg-Kirkpatrick</span>, <span>Shlomo Dubnov</span></div><span class=article-date>April, 2023</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=https://arxiv.org/abs/2211.06687 target=_blank rel=noopener>PDF</a>
<a href="index.html#" class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/clap/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header" href=https://github.com/LAION-AI/CLAP target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header" href=https://github.com/LAION-AI/audio-dataset target=_blank rel=noopener>Dataset</a></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks &ndash; text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models&rsquo; results in the non-zero-shot setting. LAION-Audio-630K and the proposed model are both available to the public.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href="../index.html#1">Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">In <em>International Conference on Acoustics, Speech and Signal Processing</em></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style><div class="alert alert-note"><div>Click the <em>Cite</em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.</div></div><div class="alert alert-note"><div>Create your slides in Markdown - click the <em>Slides</em> button to check out the example.</div></div><p><a href=https://arxiv.org/abs/2211.06687 target=_blank rel=noopener>Paper</a></p></div><div class=article-tags><a class="badge badge-light" href="../../tag/deep-learning/index.html">Deep Learning</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://tyz.netlify.app/publication/clap/&text=Large-scale%20Contrastive%20Language-Audio%20Pretraining%20with%20Feature%20Fusion%20and%20Keyword-to-Caption%20Augmentation%20" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://tyz.netlify.app/publication/clap/&t=Large-scale%20Contrastive%20Language-Audio%20Pretraining%20with%20Feature%20Fusion%20and%20Keyword-to-Caption%20Augmentation%20" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Large-scale%20Contrastive%20Language-Audio%20Pretraining%20with%20Feature%20Fusion%20and%20Keyword-to-Caption%20Augmentation%20&body=https://tyz.netlify.app/publication/clap/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://tyz.netlify.app/publication/clap/&title=Large-scale%20Contrastive%20Language-Audio%20Pretraining%20with%20Feature%20Fusion%20and%20Keyword-to-Caption%20Augmentation%20" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Large-scale%20Contrastive%20Language-Audio%20Pretraining%20with%20Feature%20Fusion%20and%20Keyword-to-Caption%20Augmentation%20%20https://tyz.netlify.app/publication/clap/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://tyz.netlify.app/publication/clap/&title=Large-scale%20Contrastive%20Language-Audio%20Pretraining%20with%20Feature%20Fusion%20and%20Keyword-to-Caption%20Augmentation%20" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://tyz.netlify.app><img class="avatar mr-3 avatar-circle" src="../../authors/admin/avatar_hu95d6bc5cb3661c6b7722816e7aa24caa_19197_270x270_fill_q75_lanczos_center.jpg" alt="Tianyu Zhang"></a><div class=media-body><h5 class=card-title><a href=https://tyz.netlify.app>Tianyu Zhang</a></h5><h6 class=card-subtitle>Ph.D. Student in Machine Learning</h6><p class=card-text>My research interests include Algorithmic Game Theory, Agent-based Model Simulator, AI for Climate Change, Multi-agent Reinforcement Learning, Self-supervised Learning, Domain Adaptation. I am still exploring and learning slowly.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:tianyu.zhang@mila.quebec><i class="fas fa-envelope"></i></a></li><li><a href="https://scholar.google.com/citations?user=jZUQSnYAAAAJ" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/tianyu-z target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://arxiv.org/a/zhang_t_6.html target=_blank rel=noopener><i class="ai ai-arxiv"></i></a></li><li><a href=https://orcid.org/0000-0002-4410-1343 target=_blank rel=noopener><i class="ai ai-orcid"></i></a></li><li><a href=https://twitter.com/tianyu_zh target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href=https://www.linkedin.com/in/tianyu-zhang-ai target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href="https://main--tyz.netlify.app/uploads/Academic_CV.pdf"><i class="ai ai-cv"></i></a></li></ul></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src="../../js/vendor-bundle.min.fab8b449b814cc9f95b22fcf2e45f05b.js"></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="https://main--tyz.netlify.app/publication/clap/{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src="https://main--tyz.netlify.app/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js" type=module></script>
<script src="../../en/js/wowchemy.min.ab2f2890dbe3e2e83579366d3d6e8fd9.js"></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href="index.html#" target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href="index.html#" target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src="../../js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type=module></script></body></html>